#include <linux/module.h>
#include <linux/highmem.h>
#include <linux/hyplet.h>
#include <linux/delay.h>
#include <linux/proc_fs.h>
#include <linux/fs.h>		/* for file_operations */
#include <linux/slab.h>	/* versioning */
#include <linux/kernel.h>
#include <linux/cdev.h>
#include "hyp_mmu.h"
#include "malware_trap.h"

static struct hyplet_driver_handler* __hyp_text is_special_address(struct hyplet_vm *vm,unsigned long addr)
{
	int i;
	struct virt_dev_access* virt = (struct virt_dev_access*)KERN_TO_HYP(vm->dev_access);
	long offset  = addr - el2_fault_address();

	for (i = 0;i < FAULT_MAX_HANDLERS; i++ ){
		if (virt->hyphnd[i].offset == offset){
			return 	&virt->hyphnd[i];
		}
	}
	return NULL;
}
/*
 * Called in EL2 to handle a faulted address
 */
int __hyp_text hyplet_handle_abrt(struct hyplet_vm *vm, unsigned long addr)
{
	struct hyplet_driver_handler* hyphnd;
	struct virt_dev_access* virt = (struct virt_dev_access*)KERN_TO_HYP(vm->dev_access);

	if (!( addr >= el2_fault_address() &&
		addr <= (el2_fault_address() + PAGE_SIZE) ) ){
			return 0;
	}
	/*
	 * An access to the virtual device detected.
	 * record the user and count
	 */
	 virt->last_current = (long)current;
	 virt->faulted_phys_addr = addr;
	 virt->count++;

/*
 *	Now we check if there is something special to do
 */
	 hyphnd  = is_special_address(vm, addr);
	 if (hyphnd){
		 	 hyphnd->action(vm, hyphnd);
	 }
	 return 1;
}


static long make_special_page_desc(unsigned long real_phyaddr,int s2_rw)
{
	unsigned long addr = real_phyaddr;
	/*
	 * 
	 * To conceal a device, we put some page.
	*  In cases where we want to monitor access to a device.
	*  we return the same address but change the access permissions
	*/
	return (DESC_AF) | (0b11 << DESC_SHREABILITY_SHIFT) |
	                ( s2_rw  << DESC_S2AP_SHIFT) | (0b1111 << 2) |
	                  DESC_TABLE_BIT | DESC_VALID_BIT | addr;
}

void stash_descriptor(unsigned long phys_addr,unsigned long* pg ,int idx)
{
	struct page *fake_pg;
	struct virt_dev_access*	 virtdev;
	struct hyplet_vm *vm;

	vm = hyplet_get_vm();

	if (phys_addr  != el2_fault_address()){
		return;
	}

	virtdev = vm->dev_access;

	fake_pg = alloc_page(GFP_KERNEL | __GFP_ZERO);
	if (!fake_pg){
		printk("Failed to allocte fake page");
	}

	/* fake page is the page that the MMIO access */
	virtdev->faddr.fake_vaddr = kmap(fake_pg);
/*
 *	Users should enter here and put the correct signature 
 *	so that a device would be instianstiated.
*/
	malware_prep_mmio((char *)virtdev->faddr.fake_vaddr);
	virtdev->faddr.fake_phys_addr = virt_to_phys(virtdev->faddr.fake_vaddr);

	printk("hyplet: creating fake page at %lx\n",
			virtdev->faddr.fake_phys_addr);

	/* the real physical address  */
	virtdev->faddr.stg2_desc_pg = pg;
	virtdev->faddr.stg2_desc_idx = idx;
	virtdev->faddr.flags = 0;
	virtdev->faddr.real_phys_addr  = phys_addr;
}

static struct proc_dir_entry *procfs = NULL;
/*
 * Input: user space virtual address, size is page size
 * Output:
 * */

static ssize_t proc_write(struct file *file, const char __user * buffer,
		  size_t count, loff_t * dummy)
{
	phys_addr_t phys_addr;
	long long virt_addr;
	long long virt_start;
	long long virt_end;
	int size = PAGE_SIZE;
	struct hyplet_vm *vm;

	vm = hyplet_get_vm();
	if (kstrtoll((void *)buffer, 16, &virt_start)) {
		printk("Failed to convert address %s",buffer);
		return -1;
	}
	printk("virt start 0x%llx\n",
			(virt_start + PAGE_SIZE) & PAGE_MASK );
	virt_end = virt_start + size;

	/*
	 * Walk over the entire address range
	 * and mark it as not accessible.
	 * */
	for (virt_addr = virt_start ;
			virt_addr < virt_end; virt_addr += PAGE_SIZE) {

		if ( __hyplet_map_user_data(virt_addr,
						PAGE_SIZE, 0 , vm) < 0){
			printk("Failed to map user data\n");
			return -1;
		}

		phys_addr = virt_to_phys((void*)virt_addr);

		make_special_page_desc(phys_addr,
				S2_PAGE_ACCESS_NONE);
	}
	return count;
}


/*
static ssize_t proc_write(struct file *file, const char __user * buffer,
			  size_t count, loff_t * dummy)
{
	struct hyplet_vm *vm;

	vm = hyplet_get_vm();

	if (vm->dev_access->faddr.stg2_desc_idx >= 0
				&& vm->dev_access->faddr.real_phys_addr
				&& vm->dev_access->faddr.stg2_desc_pg != NULL) {

		if (vm->dev_access->faddr.flags & FAULT_MMIO_TO_EL2){
			vm->dev_access->faddr.flags &= ~FAULT_MMIO_TO_EL2;
			vm->dev_access->faddr.stg2_desc_pg[vm->dev_access->faddr.stg2_desc_idx] =
						make_special_page_desc(vm->dev_access->faddr.real_phys_addr,
								S2_PAGE_ACCESS_RW);

		} else{
			struct virt_dev_access *virtdev = vm->dev_access;
			virtdev->faddr.flags |= FAULT_MMIO_TO_EL2;
			virtdev->faddr.stg2_desc_pg[virtdev->faddr.stg2_desc_idx] =
					make_special_page_desc(virtdev->faddr.fake_phys_addr,
									S2_PAGE_ACCESS_NONE);

			if (!(virtdev->faddr.flags & FAULT_MAPPED_TO_EL2)) {
				int err;

				unsigned long start = (unsigned long)virtdev->faddr.fake_vaddr;
				err = create_hyp_mappings(
									(void *)start,
									(void *)(start + PAGE_SIZE - 1),
									PAGE_HYP);
				if (err){
					printk("hyplet: failed to map to fake page\n");
					return -1;
				}
				printk("mapped to fake address to hyp at %lx\n",
						KERN_TO_HYP(vm->dev_access->faddr.fake_vaddr));
				vm->dev_access->faddr.flags |= FAULT_MAPPED_TO_EL2;
				prepare_special_addresses(vm);
			}
			mb();
		}
		return count;
	}
	vm->dev_access->faddr.flags &= ~FAULT_MMIO_TO_EL2;
	return count;
}
*/

static int proc_open(struct inode *inode, struct file *filp)
{
	filp->private_data = (void *)0x01;
	return 0;
}

static ssize_t proc_read(struct file *filp, char __user * page,
			 size_t size, loff_t * off)
{
	ssize_t len = 0;
	int cpu;
	struct hyplet_vm *vm = hyplet_get_vm();

	if ( filp->private_data == 0)
		return 0;

	len += sprintf(page + len, "Stage2: %s %s %lx  idx=%d\n",
			(vm->dev_access->faddr.flags &  FAULT_MMIO_TO_EL2) ? "on" : "off",
			(vm->dev_access->faddr.flags &  FAULT_MAPPED_TO_EL2) ? "EL2 mapped" : "EL2 not mapped",
				vm->dev_access->faddr.real_phys_addr,
				vm->dev_access->faddr.stg2_desc_idx);

	for_each_possible_cpu(cpu){
		vm = hyplet_get(cpu);
		len += sprintf(page + len, 
				"%d LastCurrent 0x%lx Addr=%lx count=%ld\n",
				cpu,
				vm->dev_access->last_current,
				vm->dev_access->faulted_phys_addr,
				vm->dev_access->count);
	}

	filp->private_data = 0x00;
	return len;
}


static struct file_operations malware_proc_ops = {
	.open = proc_open,
	.read = proc_read,
	.write = proc_write,
};




static ssize_t malware_ops_write(struct file *filp,
	const char __user *umem, size_t size, loff_t *off)
{
	struct hyplet_vm *vm =  hyplet_get_vm();
	phys_addr_t phys_addr;
	long long virt_addr;
	long long virt_start;
	long long virt_end;

	virt_start = (long long)( ((long)umem + PAGE_SIZE) & PAGE_MASK);
	virt_end = virt_start + PAGE_SIZE;
	printk("raz: virt start 0x%llx\n",
			(virt_start + PAGE_SIZE) & PAGE_MASK );
	/*
	 * Walk over the entire address range
	 * and mark it as not accessible.
	 * */
	for (virt_addr = virt_start ;
			virt_addr < virt_end; virt_addr += PAGE_SIZE) {

		if ( __hyplet_map_user_data(virt_addr,
						PAGE_SIZE, 0 , vm) < 0){
			printk("Failed to map user data\n");
			return -1;
		}

		phys_addr = virt_to_phys((void*)virt_addr);

		make_special_page_desc(phys_addr,
				S2_PAGE_ACCESS_NONE);
	}
	return size;
}

static ssize_t malware_ops_read(struct file *filp, char __user *umem, 
				size_t size, loff_t *off)
{
	int n=0;
	return size-n;
}

static struct file_operations honeypot_ops = {
	write: malware_ops_write,
	read:  malware_ops_read,
};

int honeypot_ops_major ;


void malware_init_procfs(void)
{
	procfs = proc_create_data("hyplet_stats", 
			O_RDWR, NULL, &malware_proc_ops, NULL);
	
	honeypot_ops_major = register_chrdev(0, "honeypot", &honeypot_ops);
	if (honeypot_ops_major < 0){
		printk(MODULE_NAME "Failed to create honeypot");
	}
}


